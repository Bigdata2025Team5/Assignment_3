{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# AWS S3 Configuration\n",
    "S3_BUCKET_NAME = \"bigdata2025assignment3\"\n",
    "S3_FILE_NAME = \"co2_daily.csv\"  # The file name in S3\n",
    "LOCAL_FILE_PATH = \"/Users/macbookair/Desktop/Assignment_3/co2_daily.csv\"  # Local file path\n",
    "\n",
    "# NOAA CO2 Dataset URL\n",
    "url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_daily_mlo.txt\"\n",
    "\n",
    "# Function to convert decimal year to actual date\n",
    "def decimal_to_date(decimal_year):\n",
    "    try:\n",
    "        # Extract the year and decimal part\n",
    "        year = int(decimal_year)\n",
    "        decimal_part = decimal_year - year\n",
    "        \n",
    "        # Calculate the day of the year (fraction of 365.25 days)\n",
    "        day_of_year = int(decimal_part * 365.25)\n",
    "        \n",
    "        # Calculate the start date (January 1st of the given year)\n",
    "        start_date = datetime(year, 1, 1)\n",
    "        \n",
    "        # Add the calculated day of the year to the start date\n",
    "        actual_date = start_date + timedelta(days=day_of_year - 1)  # day_of_year starts from 1\n",
    "        \n",
    "        return actual_date.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting decimal year {decimal_year} to date: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch data from the URL with error handling\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    data_lines = response.text.split(\"\\n\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logging.error(f\"Error fetching data from the URL: {e}\")\n",
    "    raise\n",
    "\n",
    "# Process the data and extract meaningful information\n",
    "data = []\n",
    "for line in data_lines:\n",
    "    if not line.startswith(\"#\") and line.strip():  # Ignore comments and empty lines\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 5:\n",
    "            try:\n",
    "                year, month, day, decimal_year, co2_value = parts[:5]\n",
    "                \n",
    "                # Convert decimal year to actual date\n",
    "                date = decimal_to_date(float(decimal_year))\n",
    "                \n",
    "                # If date is successfully converted, append the data\n",
    "                if date:\n",
    "                    co2_value = float(co2_value)\n",
    "                    data.append([date, co2_value])\n",
    "                else:\n",
    "                    logging.warning(f\"Skipping invalid date for line: {line}\")\n",
    "            except ValueError:\n",
    "                logging.warning(f\"Skipping invalid data: {line}\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"date\", \"co2_ppm\"])\n",
    "\n",
    "# Save DataFrame as a CSV file\n",
    "df.to_csv(LOCAL_FILE_PATH, index=False)\n",
    "logging.info(f\"Data saved locally as {LOCAL_FILE_PATH}\")\n",
    "\n",
    "# Upload CSV to S3 using boto3's default session\n",
    "def upload_to_s3(local_file, bucket, s3_file):\n",
    "   \n",
    "    try:\n",
    "        # Initialize the S3 client with default session (AWS credentials from environment)\n",
    "        s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=\"AKIAZPPGAAEKCP7YN7TM\",\n",
    "        aws_secret_access_key=\"7vERWy3Zl/Gec2xRcJuIJ8rCCyJip9PuJrWqQQCe\",\n",
    "        region_name=\"us-east-2\",  # Change based on your AWS region\n",
    "    )\n",
    "        # Upload the file to S3\n",
    "        s3.upload_file(local_file, bucket, s3_file)\n",
    "        logging.info(f\"File uploaded successfully to s3://{bucket}/{s3_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading file to S3: {e}\")\n",
    "        raise\n",
    "\n",
    "# Call the function to upload\n",
    "upload_to_s3(LOCAL_FILE_PATH, S3_BUCKET_NAME, S3_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1166821928.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -- ----------------------------------------------------------------------------\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SET MY_USER = CURRENT_USER();\n",
    "\n",
    "SET GITHUB_SECRET_USERNAME = 'Bigdata2025Team5';\n",
    "SET GITHUB_SECRET_PASSWORD = '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';\n",
    "SET GITHUB_URL_PREFIX = 'https://github.com/Bigdata2025Team5';\n",
    "SET GITHUB_REPO_ORIGIN = 'https://github.com/Bigdata2025Team5/Assignment_3.git';\n",
    "\n",
    "\n",
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the account level objects (ACCOUNTADMIN part)\n",
    "-- ----------------------------------------------------------------------------\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "-- Roles\n",
    "CREATE OR REPLACE ROLE CO2_ROLE;\n",
    "GRANT ROLE CO2_ROLE TO ROLE SYSADMIN;\n",
    "GRANT ROLE CO2_ROLE TO USER IDENTIFIER($MY_USER);\n",
    "\n",
    "GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE CO2_ROLE;\n",
    "GRANT EXECUTE TASK ON ACCOUNT TO ROLE CO2_ROLE;\n",
    "GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE CO2_ROLE;\n",
    "GRANT MONITOR EXECUTION ON ACCOUNT TO ROLE CO2_ROLE;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE CO2_ROLE;\n",
    "\n",
    "-- Databases\n",
    "CREATE OR REPLACE DATABASE CO2_DB;\n",
    "GRANT OWNERSHIP ON DATABASE CO2_DB TO ROLE CO2_ROLE;\n",
    "\n",
    "-- Warehouses\n",
    "CREATE OR REPLACE WAREHOUSE CO2_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\n",
    "GRANT OWNERSHIP ON WAREHOUSE CO2_WH TO ROLE CO2_ROLE;\n",
    "\n",
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the database level objects\n",
    "-- ----------------------------------------------------------------------------\n",
    "USE ROLE CO2_ROLE;\n",
    "USE WAREHOUSE CO2_WH;\n",
    "USE DATABASE CO2_DB;\n",
    "\n",
    "-- Schemas\n",
    "CREATE OR REPLACE SCHEMA INTEGRATIONS;\n",
    "CREATE OR REPLACE SCHEMA RAW_CO2;\n",
    "CREATE OR REPLACE SCHEMA Harmonized_CO2;\n",
    "CREATE OR REPLACE SCHEMA Analytics_CO2;\n",
    "\n",
    "CREATE OR REPLACE SCHEMA DEV_SCHEMA;\n",
    "CREATE OR REPLACE SCHEMA PROD_SCHEMA;\n",
    "\n",
    "\n",
    "\n",
    "CREATE OR REPLACE STAGE RAW_CO2.CO2_EXTERNAL_STAGE  \n",
    "URL = 's3://bigdata2025assignment3/co2_daily.csv'\n",
    "CREDENTIALS = (AWS_KEY_ID = 'AKIAZPPGAAEKCP7YN7TM' \n",
    "AWS_SECRET_KEY = '7vERWy3Zl/Gec2xRcJuIJ8rCCyJip9PuJrWqQQCe');\n",
    " \n",
    "-- Secrets (schema level)\n",
    "CREATE OR REPLACE SECRET DEMO_GITHUB_SECRET\n",
    "  TYPE = password\n",
    "  USERNAME = $GITHUB_SECRET_USERNAME\n",
    "  PASSWORD = $GITHUB_SECRET_PASSWORD;\n",
    "\n",
    "-- API Integration (account level)\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE OR REPLACE API INTEGRATION DEMO_GITHUB_API_INTEGRATION\n",
    "  API_PROVIDER = GIT_HTTPS_API\n",
    "  API_ALLOWED_PREFIXES = ($GITHUB_URL_PREFIX)\n",
    "  ALLOWED_AUTHENTICATION_SECRETS = (DEMO_GITHUB_SECRET)\n",
    "  ENABLED = TRUE;\n",
    "\n",
    "-- Git Repository\n",
    "CREATE OR REPLACE GIT REPOSITORY DEMO_GIT_REPO\n",
    "  API_INTEGRATION = DEMO_GITHUB_API_INTEGRATION\n",
    "  GIT_CREDENTIALS = DEMO_GITHUB_SECRET\n",
    "  ORIGIN = $GITHUB_REPO_ORIGIN;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE RAW_CO2.Daily_Measurements (\n",
    "date STRING ,\n",
    "co2_ppm FLOAT);\n",
    "\n",
    "COPY INTO RAW_CO2.Daily_Measurements\n",
    "    FROM @RAW_CO2.CO2_EXTERNAL_STAGE\n",
    "    FILE_FORMAT = (\n",
    "        TYPE = CSV \n",
    "        SKIP_HEADER = 1\n",
    "        FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "    )\n",
    "    ON_ERROR = CONTINUE;\n",
    "\n",
    "CREATE OR REPLACE STREAM RAW_CO2.DAILY_MEASUREMENTS_STREAM ON TABLE RAW_CO2.DAILY_MEASUREMENTS;\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE EVENT TABLE CO2_DB.INTEGRATIONS.DEMO_EVENTS;\n",
    "GRANT SELECT ON EVENT TABLE CO2_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE CO2_ROLE;\n",
    "GRANT INSERT ON EVENT TABLE CO2_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE CO2_ROLE;\n",
    "\n",
    "ALTER ACCOUNT SET EVENT_TABLE = CO2_DB.INTEGRATIONS.DEMO_EVENTS;\n",
    "ALTER DATABASE CO2_DB SET LOG_LEVEL = INFO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, lag, when\n",
    "from snowflake.snowpark.window import Window\n",
    " \n",
    "def calculate_co2_percentage_change(session: Session):\n",
    "    \"\"\"\n",
    "    Calculates the percentage change in CO2_PPM from the previous day.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access harmonized CO2 data table\n",
    "        harmonized_df = session.table(\"CO2_DB.HARMONIZED_CO2.CO2_EMISSIONS_HARMONIZED\")\n",
    " \n",
    "        # Define window specification ordered by 'DATE'\n",
    "        window_spec = Window.orderBy(col(\"DATE\"))\n",
    " \n",
    "        # Calculate previous day's CO2_PPM using 'lag'\n",
    "        harmonized_df = harmonized_df.with_column(\n",
    "            \"PREVIOUS_CO2\", lag(col(\"CO2_PPM\")).over(window_spec)\n",
    "        )\n",
    " \n",
    "        # Calculate percentage change from previous day\n",
    "        harmonized_df = harmonized_df.with_column(\n",
    "            \"PERCENTAGE_CHANGE\",\n",
    "            when(col(\"PREVIOUS_CO2\").isNotNull(),\n",
    "                 ((col(\"CO2_PPM\") - col(\"PREVIOUS_CO2\")) / col(\"PREVIOUS_CO2\")) * 100\n",
    "            ).otherwise(None)\n",
    "        )\n",
    " \n",
    "        # Show the result\n",
    "        harmonized_df.show()\n",
    " \n",
    "        # Save the result to a new table\n",
    "        harmonized_df.write.mode(\"overwrite\").save_as_table(\"CO2_DB.HARMONIZED_CO2.CO2_EMISSIONS_HARMONIZED_WITH_PERCENTAGE_CHANGE\")\n",
    " \n",
    "        # Return the dataframe\n",
    "        return harmonized_df\n",
    " \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_co2_percentage_change: {e}\")\n",
    "        return None\n",
    " \n",
    "# Main block to execute the script\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create Snowflake session\n",
    "        session = Session.builder.appName(\"CO2_Percentage_Change\").getOrCreate()\n",
    " \n",
    "        # Call the function to calculate percentage change\n",
    "        calculate_co2_percentage_change(session)\n",
    " \n",
    "    except Exception as e:\n",
    "        # Handle errors during session creation or transformation\n",
    "        print(f\"Error in main execution: {e}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION CO2_DB.HARMONIZED_CO2.CALCULATE_SEASONAL_VARIATION()\n",
    "RETURNS TABLE (\n",
    "    MONTH INT,\n",
    "    AVG_CO2_PPM FLOAT,\n",
    "    DEVIATION_FROM_ANNUAL_MEAN FLOAT\n",
    ")\n",
    "AS\n",
    "$$\n",
    "    WITH annual_mean AS (\n",
    "        SELECT AVG(CO2_PPM) AS mean\n",
    "        FROM CO2_DB.HARMONIZED_CO2.CO2_EMISSIONS_HARMONIZED\n",
    "    ),\n",
    "    monthly_avg AS (\n",
    "        SELECT \n",
    "            EXTRACT(MONTH FROM DATE)::INT AS MONTH,\n",
    "            AVG(CO2_PPM) AS AVG_CO2_PPM\n",
    "        FROM CO2_DB.HARMONIZED_CO2.CO2_EMISSIONS_HARMONIZED\n",
    "        GROUP BY EXTRACT(MONTH FROM DATE)\n",
    "    )\n",
    "    SELECT \n",
    "        m.MONTH,\n",
    "        m.AVG_CO2_PPM,\n",
    "        m.AVG_CO2_PPM - a.mean AS DEVIATION_FROM_ANNUAL_MEAN\n",
    "    FROM monthly_avg m, annual_mean a\n",
    "    ORDER BY m.MONTH\n",
    "$$;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
